{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c997971e-3b03-4805-8c7e-c502ee155a13",
   "metadata": {},
   "source": [
    "Exercises:\n",
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
    "\n",
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "\n",
    "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n",
    "\n",
    "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n",
    "\n",
    "E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?\n",
    "\n",
    "E06: meta-exercise! Think of a fun/interesting exercise and complete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6928f56-0d37-4539-a688-1b49f95c224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3cedfa60-1c6c-4d3b-8d4f-118987ca25b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c042bd-931d-4430-972e-20f0f259cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b4d36a-b01b-48de-9edf-f46ddbb77731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e m\n",
      "e m m\n",
      "m m a\n",
      "m a .\n"
     ]
    }
   ],
   "source": [
    "# create the training set\n",
    "xs, ys = [], []\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] +   list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        print(ch1,ch2,ch3)\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append((ix1, ix2))\n",
    "        ys.append(ix3)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c490f342-81aa-4d71-9913-b7c34c12072e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  5],\n",
       "         [ 5, 13],\n",
       "         [13, 13],\n",
       "         [13,  1]]),\n",
       " tensor([13, 13,  1,  0]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs,ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9e6ec0a-8897-47d1-8afb-bd72b6c96947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "print(xenc.shape)\n",
    "xenc = xenc.reshape((xenc.shape[0],54))\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a986fae9-16d0-4a65-8b13-a883605ed272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x157e4f5b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c56fb-c605-4a4d-aff5-ce48af82984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using one neuron, lets use 27 neurons to have a more powerful model\n",
    "# randomly initialize weights of 27 neurons. Each neuron recieves 54 inputs.\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((54,27), generator=g, requires_grad=True)\n",
    "# for example, below tells us the firing rate of the 13th neuron on the 3rd input.\n",
    "(xenc@W).shape, (xenc@W)[3,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a19da8-3fc1-4258-9369-51cd91fe0aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the output of the neural network. We need to somwhow convert these numbers to represent probabilities.\n",
    "# Probaive numbers wil become less than 1 and positive numbers will be greater than 1.\n",
    "logits = xenc@W #log-counts\n",
    "counts = logits.exp() # equivalent to the N matrix above, we can interpret these as counts.\n",
    "probs = counts/counts.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc9935-3919-40ab-a843-43c6d38acd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ef6a6-b515-4c80-aaab-8fde6652b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVERYTHING TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99800657-5c53-4a71-a619-74aafdd48864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196113, 54])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the training set\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] +   list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append((ix1, ix2))\n",
    "        ys.append(ix3)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc = xenc.reshape((xenc.shape[0],54))\n",
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c926d3b0-8202-4f88-8ad4-4455390efd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(xs, ys, test_size=0.1)\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3573cbc4-c5b3-4e2b-9d44-a2bc50f4c417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196113, 158850, 17651, 19612, torch.Size([158850, 2]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xs), len(X_train), len(X_valid), len(X_test), X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9bc1cdc-82f9-4c8d-a878-ce8f00102c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 158850\n"
     ]
    }
   ],
   "source": [
    "#initialize the network\n",
    "g = torch.Generator().manual_seed(2147)\n",
    "W = torch.randn((54,27), generator=g, requires_grad = True)\n",
    "num = X_train.shape[0]\n",
    "print(\"number of examples:\", num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "271c1d28-3cd2-420f-bd07-c5341050c6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 4.112748622894287\n",
      "cross entropy loss 4.112748146057129\n",
      "4.112748622894287\n",
      "loss 3.368908166885376\n",
      "cross entropy loss 3.368908166885376\n",
      "loss 3.062563419342041\n",
      "cross entropy loss 3.062563180923462\n",
      "loss 2.8838279247283936\n",
      "cross entropy loss 2.8838281631469727\n",
      "loss 2.7725799083709717\n",
      "cross entropy loss 2.7725796699523926\n",
      "loss 2.6940906047821045\n",
      "cross entropy loss 2.6940908432006836\n",
      "loss 2.636172294616699\n",
      "cross entropy loss 2.6361725330352783\n",
      "loss 2.591249704360962\n",
      "cross entropy loss 2.591249704360962\n",
      "loss 2.5560102462768555\n",
      "cross entropy loss 2.5560100078582764\n",
      "loss 2.527355432510376\n",
      "cross entropy loss 2.527355194091797\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(10):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(X_train, num_classes=27).float()\n",
    "    xenc = xenc.reshape((xenc.shape[0],54))\n",
    "    logits = xenc @ W #predict log counts\n",
    "    #print(logits.shape)\n",
    "    counts = logits.exp() # counts, equivalent to N matrix.\n",
    "    probs = counts/ counts.sum(1, keepdims=True) # pribability for next character\n",
    "    #print(ys.shape)\n",
    "    #print(probs.shape)\n",
    "    loss = -probs[torch.arange(num), Y_train].log().mean()\n",
    "    cross_entropy_loss = F.cross_entropy(logits, Y_train)\n",
    "    print(f\"loss {loss}\")\n",
    "    print(f\"cross entropy loss {cross_entropy_loss}\")\n",
    "    #assert torch.equal(loss, cross_entropy_loss)\n",
    "    if k%50==0:\n",
    "        print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -50*W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33812e61-5264-411a-a905-7bc3af2c53bb",
   "metadata": {},
   "source": [
    "Lets put it all inside a single function to initialize and train the network. This way, we can try different parameters for the regularization strength. Lets also create a separate function for doing a forward pass in a trained network to evaluate the validation and test losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3bb1d22-5895-42e1-8c00-87963db2fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(num_iterations, reg_strength):\n",
    "    # initialize the network\n",
    "    g = torch.Generator().manual_seed(2147)\n",
    "    W = torch.randn((54,27), generator=g, requires_grad = True)\n",
    "    num = X_train.shape[0]\n",
    "    # gradient descent\n",
    "    for k in range(num_iterations):\n",
    "        # forward pass\n",
    "        xenc = F.one_hot(X_train, num_classes=27).float()\n",
    "        xenc = xenc.reshape((xenc.shape[0],54))\n",
    "        #print(xenc.shape)\n",
    "        logits = xenc @ W #predict log counts\n",
    "        #print(logits.shape)\n",
    "        counts = logits.exp() # counts, equivalent to N matrix.\n",
    "        probs = counts/ counts.sum(1, keepdims=True) # pribability for next character\n",
    "        #print(ys.shape)\n",
    "        #print(probs.shape)\n",
    "        loss = -probs[torch.arange(num), Y_train].log().mean()+reg_strength*(W**2).mean()\n",
    "        if k%50==0:\n",
    "            print(\"loss\", loss.item())\n",
    "            print(\"-----\")\n",
    "            print(\"weights mean\", (W**2).mean())\n",
    "    \n",
    "        # backward pass\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "    \n",
    "        # update\n",
    "        W.data += -50*W.grad\n",
    "        if k%50==0:\n",
    "            print(\"*******\")\n",
    "            print(\"gradients mean\", (W.grad).mean())\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f15e52e7-ec3b-42ce-994e-a18220d954df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X,Y,W):\n",
    "    xenc = F.one_hot(X, num_classes=27).float()\n",
    "    num_samples = len(X)\n",
    "    xenc = xenc.reshape((xenc.shape[0],54))\n",
    "    #print(xenc.shape)\n",
    "    logits = xenc @ W #predict log counts\n",
    "    #print(logits.shape)\n",
    "    counts = logits.exp() # counts, equivalent to N matrix.\n",
    "    probs = counts/ counts.sum(1, keepdims=True) # pribability for next character\n",
    "    #print(ys.shape)\n",
    "    #print(probs.shape)\n",
    "    loss = -probs[torch.arange(num_samples), Y].log().mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b10ac0ab-2ff7-4f80-98ee-42d3eaaa63b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 4.1226019859313965\n",
      "-----\n",
      "weights mean tensor(1.0290, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.8299e-07)\n",
      "loss 2.299837350845337\n",
      "-----\n",
      "weights mean tensor(0.8019, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.7008e-07)\n",
      "loss 2.2706480026245117\n",
      "-----\n",
      "weights mean tensor(0.8855, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.5758e-07)\n",
      "loss 2.261986494064331\n",
      "-----\n",
      "weights mean tensor(0.9524, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.4554e-07)\n",
      "loss 2.2580580711364746\n",
      "-----\n",
      "weights mean tensor(1.0032, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.3388e-07)\n",
      "loss 2.255894899368286\n",
      "-----\n",
      "weights mean tensor(1.0430, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.2264e-07)\n",
      "loss 2.254575729370117\n",
      "-----\n",
      "weights mean tensor(1.0748, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.1174e-07)\n",
      "loss 2.2537178993225098\n",
      "-----\n",
      "weights mean tensor(1.1005, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.0123e-07)\n",
      "loss 2.2531330585479736\n",
      "-----\n",
      "weights mean tensor(1.1215, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-2.9108e-07)\n",
      "loss 2.252718687057495\n",
      "-----\n",
      "weights mean tensor(1.1387, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-2.8127e-07)\n",
      "loss 2.2524149417877197\n",
      "-----\n",
      "weights mean tensor(1.1529, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-2.7179e-07)\n",
      "loss 2.252185344696045\n",
      "-----\n",
      "weights mean tensor(1.1646, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-2.6261e-07)\n",
      "loss 2.2520077228546143\n",
      "-----\n",
      "weights mean tensor(1.1743, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-2.5374e-07)\n",
      "loss 2.2518677711486816\n",
      "-----\n",
      "weights mean tensor(1.1825, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-2.4518e-07)\n",
      "loss 2.2517547607421875\n",
      "-----\n",
      "weights mean tensor(1.1893, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-2.3693e-07)\n",
      "loss 2.251662015914917\n",
      "-----\n",
      "weights mean tensor(1.1950, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-2.2893e-07)\n",
      "loss 2.2515852451324463\n",
      "-----\n",
      "weights mean tensor(1.1998, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-2.2120e-07)\n",
      "loss 2.251521110534668\n",
      "-----\n",
      "weights mean tensor(1.2039, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-2.1376e-07)\n",
      "loss 2.251466751098633\n",
      "-----\n",
      "weights mean tensor(1.2073, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-2.0655e-07)\n",
      "loss 2.251420259475708\n",
      "-----\n",
      "weights mean tensor(1.2103, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-1.9956e-07)\n"
     ]
    }
   ],
   "source": [
    "W = train_network(1000, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7190e6d-19b0-434a-9524-fed0cc12d2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 2.2555809020996094\n",
      "test loss 2.2420644760131836\n"
     ]
    }
   ],
   "source": [
    "#Evaluate validation and test losses\n",
    "validation_loss = forward_pass(X_valid, Y_valid, W)\n",
    "test_loss = forward_pass(X_test, Y_test, W)\n",
    "\n",
    "print(f\"validation loss {validation_loss}\")\n",
    "print(f\"test loss {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95752bd5-49ae-41f6-9288-af28ef6ff092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.215212345123291\n",
      "2.349971294403076\n",
      "2.325970411300659\n",
      "2.3205535411834717\n",
      "2.318787097930908\n",
      "2.318115711212158\n",
      "2.3178393840789795\n",
      "2.3177201747894287\n",
      "2.3176662921905518\n",
      "2.317641258239746\n",
      "2.317629814147949\n",
      "2.31762433052063\n",
      "2.3176214694976807\n",
      "2.317620038986206\n",
      "2.3176193237304688\n",
      "2.3176190853118896\n",
      "2.3176190853118896\n",
      "2.3176188468933105\n",
      "2.3176188468933105\n",
      "2.3176188468933105\n"
     ]
    }
   ],
   "source": [
    "W1 = train_network(1000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8261c28e-b82b-4e13-a996-92f643b3b855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 2.2783801555633545\n",
      "test loss 2.2667903900146484\n"
     ]
    }
   ],
   "source": [
    "#Evaluate validation and test losses\n",
    "validation_loss = forward_pass(X_valid, Y_valid, W1)\n",
    "test_loss = forward_pass(X_test, Y_test, W1)\n",
    "\n",
    "print(f\"validation loss {validation_loss}\")\n",
    "print(f\"test loss {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf134223-f351-4001-b84c-3a059ee13870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 5.1413164138793945\n",
      "-----\n",
      "weights mean tensor(1.0290, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.8299e-05)\n",
      "loss 2.5447347164154053\n",
      "-----\n",
      "weights mean tensor(0.1520, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-1.0972e-06)\n",
      "loss 2.544528007507324\n",
      "-----\n",
      "weights mean tensor(0.1521, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.1444e-08)\n",
      "loss 2.544528007507324\n",
      "-----\n",
      "weights mean tensor(0.1521, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-8.9766e-10)\n",
      "loss 2.544528007507324\n",
      "-----\n",
      "weights mean tensor(0.1521, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-2.5763e-11)\n",
      "loss 2.544528007507324\n",
      "-----\n",
      "weights mean tensor(0.1521, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-6.1765e-12)\n",
      "loss 2.544528007507324\n",
      "-----\n",
      "weights mean tensor(0.1521, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-6.6431e-12)\n",
      "loss 2.544528007507324\n",
      "-----\n",
      "weights mean tensor(0.1521, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-6.1711e-12)\n",
      "loss 2.544528007507324\n",
      "-----\n",
      "weights mean tensor(0.1521, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-6.5790e-12)\n",
      "loss 2.544528007507324\n",
      "-----\n",
      "weights mean tensor(0.1521, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-6.1686e-12)\n",
      "loss 2.544528007507324\n",
      "-----\n",
      "weights mean tensor(0.1521, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-6.6451e-12)\n",
      "loss 2.544528007507324\n",
      "-----\n",
      "weights mean tensor(0.1521, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-6.1049e-12)\n",
      "loss 2.544528007507324\n",
      "-----\n",
      "weights mean tensor(0.1521, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-6.6427e-12)\n",
      "loss 2.544528007507324\n",
      "-----\n",
      "weights mean tensor(0.1521, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-6.1711e-12)\n",
      "loss 2.544528007507324\n",
      "-----\n",
      "weights mean tensor(0.1521, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-6.5790e-12)\n",
      "loss 2.544528007507324\n",
      "-----\n",
      "weights mean tensor(0.1521, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-6.1686e-12)\n",
      "loss 2.544528007507324\n",
      "-----\n",
      "weights mean tensor(0.1521, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-6.6451e-12)\n",
      "loss 2.544528007507324\n",
      "-----\n",
      "weights mean tensor(0.1521, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-6.1049e-12)\n",
      "loss 2.544528007507324\n",
      "-----\n",
      "weights mean tensor(0.1521, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-6.6427e-12)\n",
      "loss 2.544528007507324\n",
      "-----\n",
      "weights mean tensor(0.1521, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-6.1711e-12)\n"
     ]
    }
   ],
   "source": [
    "W2 = train_network(1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e82cbb93-7097-4a62-b8fd-99a1b096dc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 2.4009649753570557\n",
      "test loss 2.394542694091797\n"
     ]
    }
   ],
   "source": [
    "#Evaluate validation and test losses\n",
    "validation_loss = forward_pass(X_valid, Y_valid, W2)\n",
    "test_loss = forward_pass(X_test, Y_test, W2)\n",
    "\n",
    "print(f\"validation loss {validation_loss}\")\n",
    "print(f\"test loss {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd0d264f-cb3a-4bd6-8040-fffbc4d30bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107.01277160644531\n",
      "-----\n",
      "tensor(1.0290, grad_fn=<MeanBackward0>)\n",
      "nan\n",
      "-----\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n",
      "nan\n",
      "-----\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n",
      "nan\n",
      "-----\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n",
      "nan\n",
      "-----\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n",
      "nan\n",
      "-----\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n",
      "nan\n",
      "-----\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n",
      "nan\n",
      "-----\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n",
      "nan\n",
      "-----\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n",
      "nan\n",
      "-----\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n",
      "nan\n",
      "-----\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n",
      "nan\n",
      "-----\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n",
      "nan\n",
      "-----\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n",
      "nan\n",
      "-----\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n",
      "nan\n",
      "-----\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n",
      "nan\n",
      "-----\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n",
      "nan\n",
      "-----\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n",
      "nan\n",
      "-----\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n",
      "nan\n",
      "-----\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n",
      "nan\n",
      "-----\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "W3 = train_network(1000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6017c6d5-8d78-4a6e-9d03-7cd6bdaf15d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0141, grad_fn=<MeanBackward0>),\n",
       " tensor(-2.8622e-05, grad_fn=<MeanBackward0>),\n",
       " tensor(-8.0127e-09, grad_fn=<MeanBackward0>),\n",
       " tensor(nan, grad_fn=<MeanBackward0>),\n",
       " tensor(-0.0261, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.mean(), W1.mean(), W2.mean(), W3.mean(), W4.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "309b667b-7025-4677-83d1-e3e82aea8e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 4.113340854644775\n",
      "-----\n",
      "weights mean tensor(1.0290, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.8295e-08)\n",
      "loss 2.292961597442627\n",
      "-----\n",
      "weights mean tensor(0.8336, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.8170e-08)\n",
      "loss 2.262380599975586\n",
      "-----\n",
      "weights mean tensor(0.9432, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.8034e-08)\n",
      "loss 2.252805471420288\n",
      "-----\n",
      "weights mean tensor(1.0355, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.7895e-08)\n",
      "loss 2.2482104301452637\n",
      "-----\n",
      "weights mean tensor(1.1112, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.7774e-08)\n",
      "loss 2.245532751083374\n",
      "-----\n",
      "weights mean tensor(1.1751, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.7639e-08)\n",
      "loss 2.2438058853149414\n",
      "-----\n",
      "weights mean tensor(1.2303, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.7522e-08)\n",
      "loss 2.2426209449768066\n",
      "-----\n",
      "weights mean tensor(1.2788, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.7398e-08)\n",
      "loss 2.241769790649414\n",
      "-----\n",
      "weights mean tensor(1.3217, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.7274e-08)\n",
      "loss 2.2411351203918457\n",
      "-----\n",
      "weights mean tensor(1.3601, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.7135e-08)\n",
      "loss 2.240647077560425\n",
      "-----\n",
      "weights mean tensor(1.3947, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.7009e-08)\n",
      "loss 2.2402617931365967\n",
      "-----\n",
      "weights mean tensor(1.4260, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.6882e-08)\n",
      "loss 2.2399508953094482\n",
      "-----\n",
      "weights mean tensor(1.4546, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.6755e-08)\n",
      "loss 2.2396953105926514\n",
      "-----\n",
      "weights mean tensor(1.4807, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.6626e-08)\n",
      "loss 2.2394819259643555\n",
      "-----\n",
      "weights mean tensor(1.5049, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.6510e-08)\n",
      "loss 2.2393014430999756\n",
      "-----\n",
      "weights mean tensor(1.5272, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.6386e-08)\n",
      "loss 2.2391464710235596\n",
      "-----\n",
      "weights mean tensor(1.5480, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.6248e-08)\n",
      "loss 2.2390127182006836\n",
      "-----\n",
      "weights mean tensor(1.5674, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.6134e-08)\n",
      "loss 2.238896131515503\n",
      "-----\n",
      "weights mean tensor(1.5856, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.6005e-08)\n",
      "loss 2.2387936115264893\n",
      "-----\n",
      "weights mean tensor(1.6027, grad_fn=<MeanBackward0>)\n",
      "*******\n",
      "gradients mean tensor(-3.5856e-08)\n"
     ]
    }
   ],
   "source": [
    "W4 = train_network(1000, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78dc3b68-8b0e-4136-89af-a134a8661279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss nan\n",
      "test loss nan\n"
     ]
    }
   ],
   "source": [
    "#Evaluate validation and test losses\n",
    "validation_loss = forward_pass(X_valid, Y_valid, W3)\n",
    "test_loss = forward_pass(X_test, Y_test, W3)\n",
    "\n",
    "print(f\"validation loss {validation_loss}\")\n",
    "print(f\"test loss {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "00627169-7de9-473a-8130-e32c15810c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 2.2540059089660645\n",
      "test loss 2.240072011947632\n"
     ]
    }
   ],
   "source": [
    "#Evaluate validation and test losses\n",
    "validation_loss = forward_pass(X_valid, Y_valid, W4)\n",
    "test_loss = forward_pass(X_test, Y_test, W4)\n",
    "\n",
    "print(f\"validation loss {validation_loss}\")\n",
    "print(f\"test loss {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca66163b-e53c-4e19-a63d-85f5895bc0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.randint(0,27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ae545-4fab-452b-b718-3a1ae378c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "ix0 = 0\n",
    "ix1=random.randint(0,26)\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    while True:\n",
    "        #p = N[ix].float()\n",
    "        #p = p/p.sum()\n",
    "        #p = P[ix]\n",
    "        xenc = F.one_hot(torch.tensor((ix0,ix1)), num_classes=27).float() # input to neural network: one hot encoded\n",
    "        print(xenc.shape)\n",
    "        xenc = xenc.reshape((1,54))\n",
    "        logits = xenc @ W #predict log counts\n",
    "        counts = logits.exp() # counts, equivalent to N matrix.\n",
    "        probs = counts/ counts.sum(1, keepdims=True) # pribability for next character\n",
    "        ix0=ix1\n",
    "        ix1 = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix1])\n",
    "        if ix1 == 0: \n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fcb552-a11f-49d3-a76a-5b45a4c69c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
